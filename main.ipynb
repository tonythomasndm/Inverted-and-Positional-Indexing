{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK7KubZKQSWS",
        "outputId": "478231c5-3946-48dc-9f81-121d66917092"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CSE508_Winter2024_A1_Dataset' already exists and is not an empty directory.\n",
            "A subdirectory or file preprocessed_files already exists.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/tonythomasndm/CSE508_Winter2024_A1_Dataset.git\n",
        "! mkdir preprocessed_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xCPomgKfcZQ",
        "outputId": "2ef47df6-c76e-404b-fecb-ad531b45ebca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ttony\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ttony\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "#Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "additional_path=\"\"\n",
        "# for google collab use -/content/\n",
        "#Looping through the dataset\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  read_path=additional_path+'CSE508_Winter2024_A1_Dataset/text_files/file' + num + '.txt'\n",
        "  file_read=open(read_path,'r')\n",
        "  text=file_read.read()\n",
        "\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  sentence=\"\"\n",
        "  for token in tokens:\n",
        "    sentence=sentence+token+' '\n",
        "\n",
        "  #Saving the Preprocessed Files\n",
        "  file_write=open(additional_path+'preprocessed_files/file'+ num +'.txt','w')\n",
        "  file_write.write(sentence)\n",
        "  file_read.close()\n",
        "  file_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DipbB33NhWgk",
        "outputId": "52eab1ec-7b82-4038-ff8b-af802ab3e453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Doc 958 before preprocessing\n",
            "Suit very well if you have a mixer thats a desk top model with out any way of rack mounting it.\n",
            "\n",
            "Doc 958 after preprocessing\n",
            "suit well mixer thats desk top model way rack mounting \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 175 before preprocessing\n",
            "My ES*335 fit loose and needed some padding added to keep the guitar snug, but if you don't mind doing that it's a good deal...\n",
            "\n",
            "Doc 175 after preprocessing\n",
            "es 335 fit loose needed padding added keep guitar snug n't mind 's good deal ... \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 291 before preprocessing\n",
            "Highly recommend. It did wonders for my Kremona Solea. It's like a totally different guitar. Much more lively.\n",
            "\n",
            "Doc 291 after preprocessing\n",
            "highly recommend wonders kremona solea 's like totally different guitar much lively \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 27 before preprocessing\n",
            "Very stable and sturdy.... Well worth the $$ to protect your expensive gear\n",
            "\n",
            "Doc 27 after preprocessing\n",
            "stable sturdy .... well worth protect expensive gear \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 907 before preprocessing\n",
            "Squier Vintage Modified Jazz Bass.\n",
            "\n",
            "If your reading this you already know all the major features so I won't dig into that.\n",
            "This is an outstanding bass for the price...And even if it had a larger price tag it would still be an excellent choice.  The quality is excellent , it has great pickups and it sounds just as good as my Fender custom shop jazz.\n",
            "Don't be fooled by the price,  you don't need to spend this month's rent on a bass to get an excellent instrument that is well beyond expectations.  Add a gator hard shell case and you have a super sweet jazz bass.\n",
            "-EXCELLENT OVERALL QUALITY\n",
            "-EXCELLENT FIT AND FINISH\n",
            "-NO SHARP FRET EDGES\n",
            "-GREAT STOCK DUNCAN DESIGNED PICKUPS ..EXCELLENT\n",
            "-KILLER BASS AT TWICE THE PRICE\n",
            "Highly Recomended. You will only note one major difference if you audition the squire jazz and the fender jazz side by side. ...the decal.\n",
            "\n",
            "Doc 907 after preprocessing\n",
            "squier vintage modified jazz bass reading already know major features wo n't dig outstanding bass price ... even larger price tag would still excellent choice quality excellent great pickups sounds good fender custom shop jazz n't fooled price n't need spend month 's rent bass get excellent instrument well beyond expectations add gator hard shell case super sweet jazz bass -excellent overall quality -excellent fit finish -no sharp fret edges -great stock duncan designed pickups .. excellent -killer bass twice price highly recomended note one major difference audition squire jazz fender jazz side side ... decal \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Printing Sample 5 random documents before and after preprocessing\n",
        "import random\n",
        "\n",
        "# Generate 5 random numbers between 1 and 999\n",
        "random_numbers = random.sample(range(1, 1000), 5)\n",
        "\n",
        "for i in random_numbers:\n",
        "  num= \"% s\" % i\n",
        "  read_path=additional_path+'CSE508_Winter2024_A1_Dataset/text_files/file' + num + '.txt'\n",
        "  file_read=open(read_path,'r')\n",
        "  text=file_read.read()\n",
        "  print(\"\\nDoc \"+num+\" before preprocessing\")\n",
        "  print(text)\n",
        "  file_read.close()\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  text=file_read.read()\n",
        "  print(\"\\nDoc \"+num+\" after preprocessing\")\n",
        "  print(text)\n",
        "  file_read.close()\n",
        "  print(\"-\"*170)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RSLzHuTj8GHr"
      },
      "outputs": [],
      "source": [
        "#Creating unigram inverted index\n",
        "inverted_index={}\n",
        "total_docs=set()\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  text=file_read.read()\n",
        "  tokens=text.split()\n",
        "  doc_id=\"file\"+num+\".txt\"\n",
        "  total_docs.add(doc_id)\n",
        "  for token in tokens:\n",
        "    if token not in inverted_index:\n",
        "        inverted_index[token] = set()\n",
        "    inverted_index[token].add(doc_id)\n",
        "\n",
        "#Storing the Inverted Index\n",
        "import pickle\n",
        "f=open(additional_path+\"inverted_index.txt\",\"wb\")\n",
        "pickle.dump(inverted_index,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlBF7D05A0D_",
        "outputId": "ae6273bc-44ea-4371-8ce5-6e1a2aab6d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query 1 : live AND music\n",
            "Number of documents retrieved : 1\n",
            "Names of the documents retrieved : file22.txt, \n",
            "Query 2 : works AND NOT nicely\n",
            "Number of documents retrieved : 95\n",
            "Names of the documents retrieved : file45.txt, file703.txt, file393.txt, file773.txt, file354.txt, file645.txt, file620.txt, file356.txt, file347.txt, file285.txt, file780.txt, file30.txt, file130.txt, file173.txt, file597.txt, file448.txt, file823.txt, file487.txt, file712.txt, file317.txt, file332.txt, file968.txt, file377.txt, file686.txt, file878.txt, file605.txt, file631.txt, file724.txt, file428.txt, file568.txt, file254.txt, file50.txt, file408.txt, file184.txt, file276.txt, file897.txt, file322.txt, file2.txt, file330.txt, file390.txt, file44.txt, file261.txt, file417.txt, file945.txt, file693.txt, file580.txt, file147.txt, file353.txt, file267.txt, file344.txt, file647.txt, file90.txt, file161.txt, file684.txt, file857.txt, file293.txt, file595.txt, file600.txt, file810.txt, file368.txt, file874.txt, file801.txt, file453.txt, file886.txt, file133.txt, file776.txt, file135.txt, file406.txt, file192.txt, file784.txt, file811.txt, file994.txt, file333.txt, file323.txt, file414.txt, file708.txt, file555.txt, file632.txt, file79.txt, file867.txt, file458.txt, file644.txt, file682.txt, file760.txt, file594.txt, file35.txt, file567.txt, file467.txt, file227.txt, file709.txt, file34.txt, file450.txt, file938.txt, file563.txt, file939.txt, "
          ]
        }
      ],
      "source": [
        "#Loading the inverted Index\n",
        "f=open(additional_path+\"inverted_index.txt\",'rb')\n",
        "inverted_index = pickle.load(f)\n",
        "\n",
        "def andOp(docs1,docs2):\n",
        "    return docs1.intersection(docs2)\n",
        "\n",
        "def orOp(docs1,docs2):\n",
        "    return docs1.union(docs2)\n",
        "\n",
        "def andNotOp(docs1,docs2,total_docs):\n",
        "    not_docs2 = total_docs.difference(docs2)\n",
        "    return docs1.intersection(not_docs2)\n",
        "\n",
        "def orNotOp(docs1,docs2,total_docs):\n",
        "    not_docs2 = total_docs.difference(docs2)\n",
        "    return docs1.union(not_docs2)\n",
        "\n",
        "def queryTextPreProcess(text):\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tokensInp=[]\n",
        "operatorsInp=[]\n",
        "\n",
        "#Taking Queries\n",
        "n=int(input())\n",
        "for i in range(0,n):\n",
        "  queryText=input()\n",
        "  operators=input().lower().split(\",\")\n",
        "  operatorsInp.append(operators)\n",
        "  tokens=queryTextPreProcess(queryText)\n",
        "  tokensInp.append(tokens)\n",
        "\n",
        "#Processing Queries\n",
        "for i in range(0,n):\n",
        "\n",
        "  print(\"\\nQuery\",(i+1),\":\", end=\" \")\n",
        "  operators=operatorsInp[i]\n",
        "  tokens=tokensInp[i]\n",
        "  query=tokens[0]\n",
        "  op_count=0\n",
        "  docs=inverted_index[tokens[0]]\n",
        "  q=len(tokens)\n",
        "\n",
        "  for iQ in range(1,q):\n",
        "      op=operators[op_count].strip()\n",
        "      query=query+' '+op.upper()\n",
        "      term=tokens[iQ].strip()\n",
        "      op_count+=1\n",
        "      if term not in inverted_index:\n",
        "          inverted_index[term]=set()\n",
        "      if op == 'and':\n",
        "        docs=andOp(docs,inverted_index[term])\n",
        "      elif op =='or':\n",
        "        docs=orOp(docs,inverted_index[term])\n",
        "      elif op =='and not':\n",
        "        docs=andNotOp(docs,inverted_index[term],total_docs)\n",
        "      elif op=='or not':\n",
        "        docs=orNotOp(docs,inverted_index[term],total_docs)\n",
        "      query=query+' '+term\n",
        "  print(query)\n",
        "  print(\"Number of documents retrieved :\", len(docs))\n",
        "  if len(docs):\n",
        "    print(\"Names of the documents retrieved : \",end=\"\")\n",
        "    for i in docs:\n",
        "      print(i,end=\", \")\n",
        "  else:\n",
        "    print(\"Names of the documents retrieved: None\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GQGuzNmcu3Ni"
      },
      "outputs": [],
      "source": [
        "#Creating positional index\n",
        "positional_index={}\n",
        "total_docs=set()\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  wordPosition=1\n",
        "  text=file_read.read()\n",
        "  tokens=text.split()\n",
        "  doc_id=\"file\"+num+\".txt\"\n",
        "  total_docs.add(doc_id)\n",
        "  for token in tokens:\n",
        "    if token not in positional_index:\n",
        "        positional_index[token] ={}\n",
        "        positional_index[token]={doc_id:set()}\n",
        "    elif doc_id not in positional_index[token]:\n",
        "        positional_index[token][doc_id] = set()\n",
        "    positional_index[token][doc_id].add(wordPosition)\n",
        "    wordPosition+=1\n",
        "\n",
        "#Storing the Positional Index\n",
        "import pickle\n",
        "f=open(additional_path+\"positional_index.txt\",\"wb\")\n",
        "pickle.dump(positional_index,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDvymW8H5F6_",
        "outputId": "75c4e428-07ef-471c-a619-3f424041188b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of documents retrieved : 1\n",
            "Names of the documents retrieved : file22.txt, \n",
            "Number of documents retrieved : 2\n",
            "Names of the documents retrieved : file32.txt, file472.txt, "
          ]
        }
      ],
      "source": [
        "#Loading the positional Index\n",
        "f=open(additional_path+\"positional_index.txt\",'rb')\n",
        "inverted_index = pickle.load(f)\n",
        "\n",
        "def queryTextPreProcess(text):\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tokensInp=[]\n",
        "operatorsInp=[]\n",
        "\n",
        "#Taking Queries\n",
        "n=int(input())\n",
        "for i in range(0,n):\n",
        "  queryText=input()\n",
        "  tokens=queryTextPreProcess(queryText)\n",
        "  tokensInp.append(tokens)\n",
        "\n",
        "#Recursive Function\n",
        "docs=set()\n",
        "def fn(index,tokens,currentWordPos,currentDoc):\n",
        "  if(index==len(tokens)):\n",
        "    docs.add(currentDoc)\n",
        "    return\n",
        "  else:\n",
        "    if tokens[index] not in positional_index.keys():\n",
        "      return\n",
        "    if currentDoc in positional_index[tokens[index]]:\n",
        "      if currentWordPos+1 in positional_index[tokens[index]][currentDoc]:\n",
        "        fn(index+1,tokens,currentWordPos+1,currentDoc)\n",
        "    else:\n",
        "      return \n",
        "\n",
        "#Processing Queries\n",
        "for i in range(0,n):\n",
        "  tokens=tokensInp[i]\n",
        "  wordPosition=0\n",
        "  currentDoc=\"\"\n",
        "  for doc in positional_index[tokens[0]]:\n",
        "    for position in positional_index[tokens[0]][doc]:\n",
        "      fn(1,tokens,position,doc)\n",
        "\n",
        "  print(\"\\nNumber of documents retrieved :\", len(docs))\n",
        "  if len(docs):\n",
        "    print(\"Names of the documents retrieved : \",end=\"\")\n",
        "    for i in docs:\n",
        "      print(i,end=\", \")\n",
        "  else:\n",
        "    print(\"Names of the documents retrieved: None\")\n",
        "  docs=set()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
