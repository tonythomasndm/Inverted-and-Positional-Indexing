{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK7KubZKQSWS",
        "outputId": "478231c5-3946-48dc-9f81-121d66917092"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'CSE508_Winter2024_A1_Dataset'...\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/tonythomasndm/CSE508_Winter2024_A1_Dataset.git\n",
        "! mkdir preprocessed_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xCPomgKfcZQ",
        "outputId": "2ef47df6-c76e-404b-fecb-ad531b45ebca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ttony\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ttony\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "#Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "additional_path=\"\"\n",
        "# for google collab use -/content/\n",
        "#Looping through the dataset\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  read_path=additional_path+'CSE508_Winter2024_A1_Dataset/text_files/file' + num + '.txt'\n",
        "  file_read=open(read_path,'r')\n",
        "  text=file_read.read()\n",
        "\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  sentence=\"\"\n",
        "  for token in tokens:\n",
        "    sentence=sentence+token+' '\n",
        "\n",
        "  #Saving the Preprocessed Files\n",
        "  file_write=open(additional_path+'preprocessed_files/file'+ num +'.txt','w')\n",
        "  file_write.write(sentence)\n",
        "  file_read.close()\n",
        "  file_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DipbB33NhWgk",
        "outputId": "52eab1ec-7b82-4038-ff8b-af802ab3e453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Doc 401 before preprocessing\n",
            "A slick pick holder with a small footprint. It sticks with suction -- just press down and it adheres. It pops on and off without damaging the guitar finish so it can be switched between guitars and placed wherever most advantageous for the player. It disengages with ease for storage and/or transport. I switched out a thinner pick in place of the one included and the device gripped it tightly. Ingenious gadget and modestly priced.\n",
            "\n",
            "Doc 401 after preprocessing\n",
            "slick pick holder small footprint sticks suction -- press adheres pops without damaging guitar finish switched guitars placed wherever advantageous player disengages ease storage and/or transport switched thinner pick place one included device gripped tightly ingenious gadget modestly priced \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 866 before preprocessing\n",
            "SENNHEISER HD 8 DJ : Sennheiser these headphones aids are the best investment in money I recommend it worth it I am fascinated have a high fidelity to listen to music and to work as a DJ thanks to amazon .Djmaximo\n",
            "\n",
            "Doc 866 after preprocessing\n",
            "sennheiser hd 8 dj sennheiser headphones aids best investment money recommend worth fascinated high fidelity listen music work dj thanks amazon .djmaximo \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 990 before preprocessing\n",
            "Just got this today and I was pleasantly surprised by the whole package!  Well made case, legs and instrument.  This is my 2nd lap steel, my first was a Recording King.  I'm as pleased if not more because my RK did not have the screw on legs and has a better case and was a bit cheaper.  I will buy another in the future!\n",
            "\n",
            "Doc 990 after preprocessing\n",
            "got today pleasantly surprised whole package well made case legs instrument 2nd lap steel first recording king 'm pleased rk screw legs better case bit cheaper buy another future \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 794 before preprocessing\n",
            "Great microphone, I use it with my cheap/budget Behringer mixer, the sound is good and if I turn up the gain it will pick up everything in the room. It is great for recording acoustic guitar and vocals, don't start layering the vocals though, it starts to sound kind of weird.\n",
            "\n",
            "Doc 794 after preprocessing\n",
            "great microphone use cheap/budget behringer mixer sound good turn gain pick everything room great recording acoustic guitar vocals n't start layering vocals though starts sound kind weird \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Doc 773 before preprocessing\n",
            "Thank you\n",
            "It is a very nice little Cello.  It is for a Student about 7.  I have it all ready and tuned for Him.  The strings are breaking in very well, and the sound is getting better.  Even took it outside to let it breath.  If it works out I will be ordering more of these, for a another student.\n",
            "\n",
            "Doc 773 after preprocessing\n",
            "thank nice little cello student 7. ready tuned strings breaking well sound getting better even took outside let breath works ordering another student \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Printing Sample 5 random documents before and after preprocessing\n",
        "import random\n",
        "\n",
        "# Generate 5 random numbers between 1 and 999\n",
        "random_numbers = random.sample(range(1, 1000), 5)\n",
        "\n",
        "for i in random_numbers:\n",
        "  num= \"% s\" % i\n",
        "  read_path=additional_path+'CSE508_Winter2024_A1_Dataset/text_files/file' + num + '.txt'\n",
        "  file_read=open(read_path,'r')\n",
        "  text=file_read.read()\n",
        "  print(\"\\nDoc \"+num+\" before preprocessing\")\n",
        "  print(text)\n",
        "  file_read.close()\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  text=file_read.read()\n",
        "  print(\"\\nDoc \"+num+\" after preprocessing\")\n",
        "  print(text)\n",
        "  file_read.close()\n",
        "  print(\"-\"*170)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RSLzHuTj8GHr"
      },
      "outputs": [],
      "source": [
        "#Creating unigram inverted index\n",
        "inverted_index={}\n",
        "total_docs=set()\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  text=file_read.read()\n",
        "  tokens=text.split()\n",
        "  doc_id=\"file\"+num+\".txt\"\n",
        "  total_docs.add(doc_id)\n",
        "  for token in tokens:\n",
        "    if token not in inverted_index:\n",
        "        inverted_index[token] = set()\n",
        "    inverted_index[token].add(doc_id)\n",
        "\n",
        "#Storing the Inverted Index\n",
        "import pickle\n",
        "f=open(additional_path+\"inverted_index.txt\",\"wb\")\n",
        "pickle.dump(inverted_index,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlBF7D05A0D_",
        "outputId": "ae6273bc-44ea-4371-8ce5-6e1a2aab6d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query 1 : live AND music\n",
            "Number of documents retrieved : 1\n",
            "Names of the documents retrieved : file22.txt, \n",
            "Query 2 : works AND NOT nicely\n",
            "Number of documents retrieved : 5\n",
            "Names of the documents retrieved : file941.txt, file466.txt, file512.txt, file32.txt, file472.txt, "
          ]
        }
      ],
      "source": [
        "#Loading the inverted Index\n",
        "f=open(additional_path+\"inverted_index.txt\",'rb')\n",
        "inverted_index = pickle.load(f)\n",
        "\n",
        "def andOp(docs1,docs2):\n",
        "    return docs1.intersection(docs2)\n",
        "\n",
        "def orOp(docs1,docs2):\n",
        "    return docs1.union(docs2)\n",
        "\n",
        "def andNotOp(docs1,docs2,total_docs):\n",
        "    not_docs2 = total_docs.difference(docs2)\n",
        "    return docs1.intersection(not_docs2)\n",
        "\n",
        "def orNotOp(docs1,docs2,total_docs):\n",
        "    not_docs2 = total_docs.difference(docs2)\n",
        "    return docs1.union(not_docs2)\n",
        "\n",
        "def queryTextPreProcess(text):\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tokensInp=[]\n",
        "operatorsInp=[]\n",
        "\n",
        "#Taking Queries\n",
        "n=int(input())\n",
        "for i in range(0,n):\n",
        "  queryText=input()\n",
        "  operators=input().lower().split(\",\")\n",
        "  operatorsInp.append(operators)\n",
        "  tokens=queryTextPreProcess(queryText)\n",
        "  tokensInp.append(tokens)\n",
        "\n",
        "#Processing Queries\n",
        "for i in range(0,n):\n",
        "\n",
        "  print(\"\\nQuery\",(i+1),\":\", end=\" \")\n",
        "  operators=operatorsInp[i]\n",
        "  tokens=tokensInp[i]\n",
        "  query=tokens[0]\n",
        "  op_count=0\n",
        "  docs=inverted_index[tokens[0]]\n",
        "  q=len(tokens)\n",
        "\n",
        "  for iQ in range(1,q):\n",
        "      op=operators[op_count].strip()\n",
        "      query=query+' '+op.upper()\n",
        "      term=tokens[iQ].strip()\n",
        "      op_count+=1\n",
        "      if term not in inverted_index:\n",
        "          inverted_index[term]=set()\n",
        "      if op == 'and':\n",
        "        docs=andOp(docs,inverted_index[term])\n",
        "      elif op =='or':\n",
        "        docs=orOp(docs,inverted_index[term])\n",
        "      elif op =='and not':\n",
        "        docs=andNotOp(docs,inverted_index[term],total_docs)\n",
        "      elif op=='or not':\n",
        "        docs=orNotOp(docs,inverted_index[term],total_docs)\n",
        "      query=query+' '+term\n",
        "  print(query)\n",
        "  print(\"Number of documents retrieved :\", len(docs))\n",
        "  if len(docs):\n",
        "    print(\"Names of the documents retrieved : \",end=\"\")\n",
        "    for i in docs:\n",
        "      print(i,end=\", \")\n",
        "  else:\n",
        "    print(\"Names of the documents retrieved: None\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GQGuzNmcu3Ni"
      },
      "outputs": [],
      "source": [
        "#Creating positional index\n",
        "positional_index={}\n",
        "total_docs=set()\n",
        "for i in range(1,1000):\n",
        "  num= \"% s\" % i\n",
        "  file_read=open(additional_path+'preprocessed_files/file'+ num +'.txt','r')\n",
        "  wordPosition=1\n",
        "  text=file_read.read()\n",
        "  tokens=text.split()\n",
        "  doc_id=\"file\"+num+\".txt\"\n",
        "  total_docs.add(doc_id)\n",
        "  for token in tokens:\n",
        "    if token not in positional_index:\n",
        "        positional_index[token] ={}\n",
        "        positional_index[token]={doc_id:set()}\n",
        "    elif doc_id not in positional_index[token]:\n",
        "        positional_index[token][doc_id] = set()\n",
        "    positional_index[token][doc_id].add(wordPosition)\n",
        "    wordPosition+=1\n",
        "\n",
        "#Storing the Positional Index\n",
        "import pickle\n",
        "f=open(additional_path+\"positional_index.txt\",\"wb\")\n",
        "pickle.dump(positional_index,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDvymW8H5F6_",
        "outputId": "75c4e428-07ef-471c-a619-3f424041188b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of documents retrieved : 1\n",
            "Names of the documents retrieved : file22.txt, \n",
            "Number of documents retrieved : 2\n",
            "Names of the documents retrieved : file32.txt, file472.txt, "
          ]
        }
      ],
      "source": [
        "#Loading the positional Index\n",
        "f=open(additional_path+\"positional_index.txt\",'rb')\n",
        "inverted_index = pickle.load(f)\n",
        "\n",
        "def queryTextPreProcess(text):\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Remove punctuations\n",
        "  tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "  # Remove blank space tokens\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tokensInp=[]\n",
        "operatorsInp=[]\n",
        "\n",
        "#Taking Queries\n",
        "n=int(input())\n",
        "for i in range(0,n):\n",
        "  queryText=input()\n",
        "  tokens=queryTextPreProcess(queryText)\n",
        "  tokensInp.append(tokens)\n",
        "\n",
        "#Recursive Function\n",
        "docs=[]\n",
        "def fn(index,tokens,currentWordPos,currentDoc):\n",
        "  if(index==len(tokens)):\n",
        "    docs.append(currentDoc)\n",
        "    return\n",
        "  else:\n",
        "    if tokens[index] not in positional_index.keys():\n",
        "      return\n",
        "    if currentDoc in positional_index[tokens[index]]:\n",
        "      if currentWordPos+1 in positional_index[tokens[index]][currentDoc]:\n",
        "        fn(index+1,tokens,currentWordPos+1,currentDoc)\n",
        "    else:\n",
        "      return \n",
        "\n",
        "#Processing Queries\n",
        "for i in range(0,n):\n",
        "  tokens=tokensInp[i]\n",
        "  wordPosition=0\n",
        "  currentDoc=\"\"\n",
        "  for doc in positional_index[tokens[0]]:\n",
        "    for position in positional_index[tokens[0]][doc]:\n",
        "      fn(1,tokens,position,doc)\n",
        "\n",
        "  print(\"\\nNumber of documents retrieved :\", len(docs))\n",
        "  if len(docs):\n",
        "    print(\"Names of the documents retrieved : \",end=\"\")\n",
        "    for i in docs:\n",
        "      print(i,end=\", \")\n",
        "  else:\n",
        "    print(\"Names of the documents retrieved: None\")\n",
        "  docs=[]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
